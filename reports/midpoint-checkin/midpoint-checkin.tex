\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   	% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
\usepackage[parfill]{parskip}    	% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or eps§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{amsmath}
\usepackage{biblatex}
\addbibresource{citations.bib}
%SetFonts

%SetFonts


\title{Efficient Computer Vision: Pushing the Frontier of Edge Computing}
\author{Michael Krah, Zach Gentile, and Alex Lavaee}
\date{April 6, 2025}

\begin{document}
\maketitle
\begin{abstract}
	The Low Power Computer Vision Challenge (LPCVC) emphasizes the importance of optimizing models for efficient deployment on edge devices. Low-power, efficient computer vision is critical for applications requiring real-time processing with limited energy and hardware resources, such as mobile and IoT devices. Efficient models enable widespread deployment, reduce energy consumption, and enhance privacy by processing data locally rather than relying on cloud-based solutions. Inspired by LPCVC, our project investigates advanced optimization methods tailored for edge devices. We explore architectural improvements including quantization and token-efficient approaches, alongside training enhancements such as 8-bit General Matrix Multiplication (GEMM) and Multi-head Latent Attention (MLA). Our solution aims to be lightweight, efficient, and optimized for direct execution on edge devices with limited hardware.


\end{abstract}

\section*{Introduction}

Computer vision at the edge represents a critical advancement in artificial intelligence, enabling real-time image analysis on resource-constrained devices without reliance on cloud connectivity. The 2025 IEEE Low-Power Computer Vision Challenge (LPCVC) highlights the importance of optimizing computer vision models for deployment on low-power, edge-based hardware, focusing specifically on image classification across varying lighting conditions and artistic styles. Ideally, smaller models could be run locally on portable devices to assist in real-time classification.

While our project does not directly participate in LPCVC, it is deeply inspired by the competition’s goals and methodologies. Our work seeks to explore and extend the competition’s emphasis on efficient, robust, and lightweight models tailored for edge deployment. Specifically, we aim to address similar challenges outlined by LPCVC by investigating innovative neural network architectures and optimization strategies, including quantization, token-efficient approaches, and specialized training methods such as 8-bit General Matrix Multiplication (GEMM) and Multi-head Latent Attention (MLA).

Our objective is to advance state-of-the-art practices in efficient computer vision, balancing the trade-off between classification accuracy, inference speed, and robustness to diverse visual conditions. By drawing inspiration from LPCVC’s structured approach to benchmarking, we aim to develop solutions beneficial for researchers and developers working within the constraints of limited computational resources, with potential applications spanning augmented reality, autonomous systems, and smart devices. Additionally, we prioritize creating comprehensive documentation to ensure our methods are accessible, well-explained, and easy to replicate. This documentation is intended to guide future users, researchers, and practitioners interested in learning, implementing, and extending the optimization strategies and architectures explored in this project.



\section*{Related Work}

Efficient computer vision for edge devices has seen significant advancements across several key areas. We organize our literature review around three main themes particularly relevant to the LPCVC challenge:


\subsection*{Model Compression and Quantization}

Neural network compression has become crucial for deploying deep learning models on resource-constrained devices. Work by Han et al. \cite{han2016deepcompressioncompressingdeep} introduced deep compression, combining pruning, quantization, and Huffman coding to significantly reduce model size while maintaining accuracy. Building on this foundation, Jacob et al. \cite{jacob2017quantizationtrainingneuralnetworks} developed quantization-aware training techniques that allow for 8-bit integer operations without significant accuracy degradation. More recently, Nagel et al. \cite{nagel2021whitepaperneuralnetwork} demonstrated data-free quantization methods that enable post-training conversion to lower precision formats, making deployment more accessible.

\subsection*{Architectures for Edge Deployment}

Specialized neural network architectures designed explicitly for edge devices have shown remarkable efficiency. MobileNetV2 \cite{sandler2019mobilenetv2invertedresidualslinear} and MobileNetV3 \cite{howard2019searchingmobilenetv3} introduced inverted residuals and linear bottlenecks that dramatically reduced computational costs. EfficientNet \cite{tan2020efficientnetrethinkingmodelscaling} proposed a principled scaling method that optimizes depth, width, and resolution dimensions simultaneously. Most relevant to our work, Lin et al. \cite{lin2020mcunettinydeeplearning} developed MCUNet for ultra-small microcontrollers, demonstrating techniques to adapt neural networks to extremely limited hardware.

\subsection*{1D Image Tokenization}

Recent advancements have explored alternative image representation methods, notably the work by Yu et al. \cite{yu2024imageworth32tokens} which introduced TiTok, a transformer-based 1-Dimensional tokenizer. TiTok significantly reduces image representations to compact 1D latent sequences, achieving efficient tokenization using as few as 32 tokens for a 256×256×3 image. This approach effectively handles redundancies in images, significantly improving computational efficiency in image reconstruction and generation tasks. Despite its compact form, TiTok outperforms state-of-the-art models on benchmarks like ImageNet, providing faster generation speeds and competitive generative quality.

% \subsection*{Robustness to Lighting and Style Variations}

% Addressing variations in lighting conditions has been a longstanding challenge in computer vision. Histogram equalization and other traditional techniques have been supplemented by learning-based approaches as described by Lore et al. \cite{lore2016llnetdeepautoencoderapproach} who developed LLNet specifically for low-light image enhancement. For style robustness, Geirhos et al. \cite{geirhos2022imagenettrainedcnnsbiasedtexture} demonstrated that CNNs are biased toward texture rather than shape and proposed methods to improve generalization across styles. 

\section*{Current Work and Preliminary Results}

Our current model leverages a token efficient vision transformer, TiTok \cite{yu2024imageworth32tokens}, as a base model for image classification. We have recreated the paper's pretraining and training pipeline to reproduce established results and finetune the model for classification. 

\begin{itemize}
    \item \textbf{Dataset Implementation:} ImageNet was loaded to the SCC and then converted to the WebDataSet format. Images were then tokenized using MaskGIT to generate "proxy codes" to be later used in training\cite{chang2022maskgitmaskedgenerativeimage}, following the training strategy implemented by \cite{yu2024imageworth32tokens}. This gave us access to more than 1 million images on the SCC for training.  
    \item \textbf{Pretraining:} TiTok split training for the encoder and decoder into two distinct phases. Encoder training uses the "proxy code" tokens generated by MaskGIT as a desired output, ensuring stability of the model. 
    \item \textbf{Linear Probe:} To verify TiTok's results and for a classification pipeline, we implemented linear probing. We followed the approach of  \cite{he2021maskedautoencodersscalablevision}, modifying their given code to use the Webdataset format and to be compatible with the TiTok model. To obtain preliminary results, we have added a linear layer to TiTok's encoder and frozen all layers except for the head.

\end{itemize}


\section*{Proposed Work}

Our solution inspired by 2025 LPCVC Track 1 challenge builds on state-of-the-art techniques in efficient deep learning and leverages insights from recent low-power computer vision studies \cite{chen20242023lowpowercomputervision}. Our approach combines algorithmic innovations with hardware-specific optimizations to achieve superior accuracy and speed on edge devices.

\subsection*{Model Architecture and Optimization}

We begin with a lightweight backbone based on TiTok \cite{yu2024imageworth32tokens}, enhanced with attention mechanisms inspired by transformer architectures \cite{dosovitskiy2021imageworth16x16words,touvron2021trainingdataefficientimagetransformers} and efficient mobile designs \cite{pan2022edgevitscompetinglightweightcnns}. Our approach includes:

\begin{itemize}
    \item \textbf{Token-Efficient Vision Transformers:} We incorporate token-efficient strategies to further reduce computational overhead. Inspired by \cite{yu2024imageworth32tokens}, our design represents images using only 32 tokens, drastically reducing the input dimensionality.
    
	\item \textbf{Mixed-Precision Training:} We implement dynamic quantization that maintains full precision for critical parameters while quantizing less sensitive components. This approach is guided by prior work on efficient integer-arithmetic-only inference \cite{jacob2017quantizationtrainingneuralnetworks}.
	
	\item \textbf{8-bit Matrix Multiplication:} Our model utilizes 8-bit integer matrix multiplication (Int8 MatMul) with calibrated scaling factors to minimize quantization error, reducing memory and computational complexity by approximately 75\% compared to FP32 operations \cite{jacob2017quantizationtrainingneuralnetworks}.
	
	
\end{itemize}

\subsection*{Hardware-Specific Optimizations}

To fully leverage hardware capabilities, we propose several hardware-specific enhancements:

\begin{itemize}
	\item \textbf{Custom CUDA Kernels for GPU Optimization:} We will develop custom CUDA kernels tailored for NVIDIA GPUs, leveraging parallel processing capabilities to optimize convolutional operations. This approach aligns with recent advancements in GPU-based hardware acceleration and software optimization, maximizing computational efficiency and performance. \cite{deepseekai2025deepseekv3technicalreport}.
	
	\item \textbf{GEMM and Multi-Head Latent Attention (MLA):} In addition to custom kernels, we integrate high-performance General Matrix Multiplication (GEMM) routines alongside novel Multi-head Latent Attention (MLA) mechanisms, as introduced in \cite{deepseekai2025deepseekv3technicalreport}. GEMM optimizations streamline batched computations, while MLA enhances the attention mechanism by leveraging latent representations, thus reducing computational overhead and improving feature extraction.
	
\end{itemize}

\subsection*{Robustness to Image Variability and Underrepresented Classes}

Given the variability that is inherent to photographing and classifying images, we aim to test the robustness of our model in predicting underrepresented classes or images that may be taken in variable conditions:

\begin{itemize}
	\item \textbf{Data Augmentation Strategy:} A comprehensive augmentation pipeline will simulate diverse lighting conditions and artistic styles during training, enhancing the model’s generalization.
	
	
	\item \textbf{Domain Adaptation Techniques:} Adversarial domain adaptation components are incorporated to minimize feature distribution shifts between different lighting and style domains.
\end{itemize}

To score our final model's performance, we hope to adapt the LPCVC's formula for calculating score \cite{chen20242023lowpowercomputervision}. This will allow of to balance accuracy and speed when determining the performance of the model. We also plan to use an F1-score to determine the model's overrall performance, especially in regards to underrrepresented classes.

\begin{equation}
\label{eq:finalScore}
	\text{Score} = \frac{\text{Accuracy}}{\max(\text{ExecutionTime}/2, 1\text{ms})}. 
\end{equation}




\section*{Datasets}

Our project leverages the original ImageNet dataset for training, with comprehensive evaluation conducted using the ImageNet, ImageNet-C, and ImageNet-P datasets. These datasets collectively enable robust assessment of both in-distribution and out-of-distribution performance.

\subsection*{Dataset Composition}

The ImageNet dataset comprises approximately 1.28 million training images across 1,000 diverse object classes, ranging from animals and plants to household and industrial items. 

\subsection*{Corruptions and Perturbations (ImageNet-C and ImageNet-P)}

For evaluating model robustness to common corruptions and perturbations, we employ ImageNet-C and ImageNet-P, two benchmarks introduced by Hendrycks and Dietterich \cite{hendrycks2019benchmarkingneuralnetworkrobustness}.

\textbf{ImageNet-C} contains validation images corrupted by 15 diverse types of visual corruptions across four categories:

\begin{itemize}
\item \textbf{Noise:} Gaussian, shot, impulse noise
\item \textbf{Blur:} defocus, frosted glass, motion, zoom blur
\item \textbf{Weather:} snow, frost, fog, brightness variations
\item \textbf{Digital:} contrast changes, elastic transformations, pixelation, JPEG compression artifacts
\end{itemize}

Each corruption type is presented at five severity levels, enabling precise measurement of robustness across varying degrees of visual distortions.

\textbf{ImageNet-P} evaluates perturbation robustness through sequences of images subjected to subtle transformations. This benchmark captures the stability of model predictions under minute perturbations, including:

\begin{itemize}
\item Gaussian and shot noise perturbations
\item Motion and zoom blur perturbations
\item Weather-based perturbations (e.g., snow, brightness changes)
\item Spatial perturbations (translations, rotations, viewpoint tilts, scaling)
\end{itemize}

Each perturbation sequence consists of multiple frames with incremental transformations, measuring how consistently the model maintains correct classifications.

\subsection*{Dataset Format and Preprocessing}

All images from these datasets are standardized to RGB format at a resolution of 224×224 pixels. The input tensors are structured with dimensions (batch, 3, 224, 224) and pixel values normalized to the [0, 1] range as floating-point values. Our preprocessing pipeline includes:

\begin{itemize}
\item Data normalization using ImageNet's mean and standard deviation values
\item Data augmentation through random cropping, horizontal flipping, and color jittering
\item Additional augmentation techniques to simulate corruption and perturbation scenarios
\end{itemize}

\subsection*{Dataset Split and Evaluation}

For evaluation and testing we will be using ImageNet datasets. The base Titok model was trained on ImageNet and we plan to use the same dataset to refine and evaluate the model. Evaluation will be done on Imagenet-C/P as well as the original ImageNet. 

The variety of classes in ImageNet and the modifications provided in ImageNet-C and Imagenet-P provide a wide range of test data to train that can be used to evaluate the adaptability of the model.

\section*{Evaluation}

Our evaluation methodology follows the official metrics for the 2025 LPCVC Track 1 challenge while incorporating additional analyses to guide our development process. 

\subsection*{Primary Metrics}

In accordance with the goals initially established by the challenge, our primary evaluation metrics are:

\begin{itemize}
	\item \textbf{Execution Time:} Our solution will seek to minimize CPU and GPU inference time. To have been considered valid for the LPCVC competition, we would have to execute within 10ms per image on the Snapdragon 8 Elite QRD platform .

	\item \textbf{Classification Accuracy:} The percentage of correctly classified images across all 1000 classes in ImageNet.

\end{itemize}

We plan to use classification results from the TiTok model \cite{yu2024imageworth32tokens} as a baseline and work to reproduce and improve this score. 

\subsection*{Development Evaluation Strategy}

During the development process, we will implement a more comprehensive evaluation approach:

\begin{itemize}

	\item \textbf{Validation Set Evaluation:} We will measure performance on the validation representing corrupted and perturbed versions of ImageNet.

	\item \textbf{Confusion Matrix Analysis:} We will analyze the confusion matrix to identify frequently misclassified class pairs and target these specifically in our model refinement.

	\item \textbf{Performance Profiling:} We will profile memory usage, CPU/GPU/DSP utilization, and power consumption to guide our optimization efforts.
\end{itemize}

\subsection*{Baseline Comparisons}

We will establish the following baselines for comparison:

\begin{itemize}

	\item \textbf{Standard MobileNetV2:} We will evaluate a standard MobileNetV2 model to understand the improvements offered by our specialized approach.

    \item \textbf{TiTok-Tokenizer:} As we build upon this architecture, we will track improvements over the original implementation.

\end{itemize}


\section*{Timeline}

We will structure our project with the following timeline:

\subsection*{Phase 1: Research and Exploration (Week 1-2, March 2-16)}

\begin{itemize}
	\item Literature review of efficient vision models and optimization techniques
    \item Find base architecture to iterate on
	\item Setup development environment
\end{itemize}

\subsection*{Phase 2: Development and Optimization (Weeks 3-7, March 17-April 13)}

\begin{itemize}
	\item Convert Imagenet dataset to WebDataSet format
    \item Run Pretokenization to generate proxy codes for TiTok model training
    \item Implementation of core model architecture (TiTok), run Linear Probing and achieve baseline
	\item Exploring model hyperparameters
	\item Comprehensive evaluation and performance tuning
    \item Explore quantization, 8-bit General Matrix Multiplication, and Multi-head Latent Attention
\end{itemize}

\subsection*{Phase 3: Refinement and Submission (Week 8-10, April 14-30)}

\begin{itemize}
	\item Continue refining algorithm, model, and hyperparameters
	\item Final model benchmarking and validation
	\item Documentation preparation and code cleanup
    \item Write-up sharing our insights and common techniques to increase inference speed
    \item Create demo of model running on iPhones using the MLX framework (if time) \cite{mlx2023}
	\item Preparation of final project paper
\end{itemize}

% \subsection*{Key Milestones}

% \begin{itemize}
% 	\item March 10: Finish literature review and setup development environment
% 	\item March 17: Implement baseline, core model architecture, and develop initial codebase
% 	\item March 31: Submit working version of model to IEEE Lower-Power Computer Vision Challenge
% 	\item April 15: Finalize models and prepare paper and presentation
% 	\item May 1: Submit final code, paper, and presentation
% \end{itemize}

\section*{Conclusion}

This document outlines our approach inspired by the 2025 IEEE Low-Power Computer Vision Challenge - Track 1, focusing on efficient image classification robust to lighting variations and artistic styles. Our strategy leverages token-efficient methods alongside planned implementations of quantization, Multi-head Latent Attention, 8-bit General Matrix Multiplication. and optimized CUDA kernels to achieve both high accuracy and low inference latency, thereby enhancing the accessibility and practicality of deep learning models on edge devices.

    % Our strategy leverages 8-bit matrix multiplication, structured pruning, knowledge distillation, and custom optimized kernels to achieve both high accuracy and low inference latency for improving the accessibility of Deep Learning models.



\printbibliography

\end{document}
