pretrained_model_name_or_path: "yucornetto/tokenizer_titok_s128_imagenet" # pretrained model name or path
experiment_name: "linear_probe_titok_s128_imagenet"
wandb_entity: "artificial-intelligence-research"
wandb_project: "eff-cv"

base_dir: "/projectnb/dl4ds/students/alavaee/outputs"

# Training parameters
batch_size: 4096 # Batch size per GPU (effective batch size is batch_size * accum_iter * # gpus)
epochs: 90
accum_iter: 1 # Accumulate gradient iterations (for increasing the effective batch size under memory constraints)

# Optimizer parameters
weight_decay: 0 # Weight decay (default: 0 for linear probe following MoCo v1)
lr: null # Learning rate (absolute lr)
blr: 0.1 # Base learning rate: absolute_lr = base_lr * total_batch_size / 256
min_lr: 0.0 # Lower lr bound for cyclic schedulers that hit 0
warmup_epochs: 10 # Epochs to warmup LR

# Finetuning parameters
finetune: "" # Finetune from checkpoint

# Dataset parameters
data_path: "/projectnb/dl4ds/materials/datasets/imagenet" # Dataset path
train_pattern: "imagenet-train-{000000..000319}.tar" # WebDataset train pattern
val_pattern: "imagenet-val-{000000..000049}.tar" # WebDataset val pattern
nb_classes: 1000 # Number of the classification types

# Output parameters
output_dir: "${base_dir}/${experiment_name}" # Path where to save, empty for no saving
log_dir: "${base_dir}/${experiment_name}" # Path where to tensorboard log
device: "cuda" # Device to use for training / testing
seed: 0

# Checkpoint parameters
resume: "" # Resume from checkpoint
start_epoch: 0 # Start epoch

# Evaluation parameters
eval: false # Perform evaluation only
dist_eval: false # Enabling distributed evaluation (recommended during training for faster monitor)

# DataLoader parameters
num_workers: 4
pin_mem: true # Pin CPU memory in DataLoader for more efficient (sometimes) transfer to GPU

# Distributed training parameters
world_size: 1 # Number of distributed processes
local_rank: -1
dist_on_itp: false
dist_url: "env://" # URL used to set up distributed training
