==========================================================
Start date : Mon Apr  7 11:53:27 EDT 2025
Job name : linear_probe
Job ID : 3564440
Host : 
==========================================================
attention mode is flashattention mode is flash

| distributed init (rank 0): env://, gpu 0
| distributed init (rank 1): env://, gpu 1
[11:53:55.264212] job dir: /projectnb/dl4ds/students/alavaee/efficient-cv/scripts/train
[11:53:55.264334] {'pretrained_model_name_or_path': 'yucornetto/tokenizer_titok_s128_imagenet',
'experiment_name': 'linear_probe_titok_s128_imagenet',
'wandb_entity': 'artificial-intelligence-research',
'wandb_project': 'eff-cv',
'base_dir': '/projectnb/dl4ds/students/alavaee/outputs',
'batch_size': 4096,
'epochs': 90,
'accum_iter': 1,
'weight_decay': 0,
'lr': None,
'blr': 0.1,
'min_lr': 0.0,
'warmup_epochs': 10,
'finetune': '',
'data_path': '/projectnb/dl4ds/materials/datasets/imagenet',
'train_pattern': 'imagenet-train-{000000..000319}.tar',
'val_pattern': 'imagenet-val-{000000..000049}.tar',
'nb_classes': 1000,
'output_dir': '${base_dir}/${experiment_name}',
'log_dir': '${base_dir}/${experiment_name}',
'device': 'cuda',
'seed': 0,
'resume': '',
'start_epoch': 0,
'eval': False,
'dist_eval': False,
'num_workers': 4,
'pin_mem': True,
'world_size': 2,
'local_rank': -1,
'dist_on_itp': False,
'dist_url': 'env://',
'config': 'configs/training/finetune/linear_probe.yaml',
'rank': 0,
'gpu': 0,
'distributed': True,
'dist_backend': 'nccl'}
wandb: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.
wandb: Currently logged in as: lavaalex to https://api.wandb.ai. Use `wandb login --relogin` to force relogin
wandb: Tracking run with wandb version 0.19.8
wandb: Run data is saved locally in /projectnb/dl4ds/students/alavaee/efficient-cv/wandb/run-20250407_115355-onig4v9o
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run linear_probe_titok_s128_imagenet
wandb: ‚≠êÔ∏è View project at https://wandb.ai/artificial-intelligence-research/eff-cv
wandb: üöÄ View run at https://wandb.ai/artificial-intelligence-research/eff-cv/runs/onig4v9o
[11:53:57.095008] Creating WebDataset loaders: /projectnb/dl4ds/materials/datasets/imagenet/imagenet-train-{000000..000319}.tar and /projectnb/dl4ds/materials/datasets/imagenet/imagenet-val-{000000..000049}.tar
[11:53:58.133914] Model = TiTokEncoder(
  (patch_embed): Conv2d(3, 512, kernel_size=(16, 16), stride=(16, 16))
  (ln_pre): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (transformer): ModuleList(
    (0-7): 8 x ResidualAttentionBlock(
      (ln_1): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (attn): MultiheadAttention(
        (out_proj): NonDynamicallyQuantizableLinear(in_features=512, out_features=512, bias=True)
      )
      (ln_2): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
      (mlp): Sequential(
        (c_fc): Linear(in_features=512, out_features=2048, bias=True)
        (gelu): GELU(approximate='none')
        (c_proj): Linear(in_features=2048, out_features=512, bias=True)
      )
    )
  )
  (ln_post): LayerNorm((512,), eps=1e-05, elementwise_affine=True)
  (conv_out): Conv2d(512, 12, kernel_size=(1, 1), stride=(1, 1))
  (fc_norm): LayerNorm((512,), eps=1e-06, elementwise_affine=True)
  (head): Sequential(
    (0): BatchNorm1d(512, eps=1e-06, momentum=0.1, affine=False, track_running_stats=True)
    (1): Linear(in_features=512, out_features=1000, bias=True)
  )
)
[11:53:58.134492] number of params (M): 0.51
[11:53:58.134818] base lr: 1.00e-01
[11:53:58.134990] actual lr: 3.20e+00
[11:53:58.135144] accumulate grad iterations: 1
[11:53:58.135283] effective batch size: 8192
[11:53:58.140693] LARS (
Parameter Group 0
    lr: 3.2
    momentum: 0.9
    trust_coefficient: 0.001
    weight_decay: 0
)
[11:53:58.141074] criterion = CrossEntropyLoss()
[11:53:58.141288] Start training for 90 epochs
[11:54:31.785236] Epoch: [0]  [ 0/40]  eta: 0:22:25  lr: 0.000000  loss: 6.9334 (6.9334)  time: 33.6423  data: 27.2009  max mem: 14284
[11:56:13.556325] Epoch: [0]  [20/40]  eta: 0:02:08  lr: 0.160000  loss: 6.9268 (6.9288)  time: 5.0885  data: 4.2951  max mem: 14302
[11:57:51.121853] Epoch: [0]  [39/40]  eta: 0:00:05  lr: 0.312000  loss: 6.9026 (6.9162)  time: 5.6620  data: 4.2700  max mem: 14302
[11:57:51.126710] Epoch: [0] Total time: 0:03:52 (5.8246 s / it)
[11:57:51.127812] Averaged stats: lr: 0.312000  loss: 6.9026 (6.9165)
[11:58:31.847506] Test:  [0/2]  eta: 0:01:21  loss: 6.8819 (6.8819)  acc1: 0.0732 (0.0732)  acc5: 0.9521 (0.9521)  time: 40.5052  data: 38.0391  max mem: 14302
[11:58:32.497962] Test:  [1/2]  eta: 0:00:20  loss: 6.8819 (6.8830)  acc1: 0.0732 (0.1343)  acc5: 0.8057 (0.8789)  time: 20.5772  data: 19.0197  max mem: 14302
[11:58:51.268607] Test: Total time: 0:00:59 (29.9633 s / it)
[11:58:52.422507] * Acc@1 0.134 Acc@5 0.879 loss 6.883
[11:58:52.423090] Accuracy of the network on the 50000 test images: 0.1%
[11:58:52.423276] Max accuracy: 0.13%
[11:59:21.129926] Epoch: [1]  [ 0/40]  eta: 0:19:07  lr: 0.320000  loss: 6.8784 (6.8784)  time: 28.6837  data: 25.7488  max mem: 14302
